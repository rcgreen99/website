<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <link rel="icon" href="/docs/4.0/assets/img/favicons/favicon.ico" />

    <!-- Bootstrap core CSS -->
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
      integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
      crossorigin="anonymous"
    />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css"
    />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/icon?family=Material+Icons"
    />
    <link rel="stylesheet" href="../static/style.css" />
  </head>

  <body>
    <div id="page-container">
      <div id="content-wrap">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-md-10">
              <div class="blog-container">
                <h1>
                  The Great Debate: Sutskever vs. Chollet on the Future of AGI
                </h1>
                <i>June 19, 2024</i>
                <br />
                <br />
                <div class="text-center">
                  <img
                    src="../static/images/ilya-chollet.jpg"
                    alt="Ilya vs Chollet"
                    class="img-fluid rounded"
                  />
                </div>
                <br />
                <p>
                  Two of the most influential figures in AI fundamentally
                  disagree on the path to AGI. Ilya Sutskever, former Chief
                  Scientist of OpenAI
                  <a
                    href="https://www.bloomberg.com/news/articles/2024-06-19/openai-co-founder-plans-new-ai-focused-research-lab?srnd=all"
                    >and now Chief Scientist at Safe Superintelligence</a
                  >
                  thinks that if we keep scaling up LLM's, we'll be very close
                  to achieving AGI. On the other hand, François Chollet, creator
                  of Keras, thinks that LLM's alone aren't enough.
                </p>

                <p>
                  Both Sutskever and Chollet have gone on
                  <a href="https://www.youtube.com/@DwarkeshPatel"
                    >Dwarkesh Patel's podcast</a
                  >, where they've discussed their views on LLM's and AGI. It
                  should be noted that Ilya's interview took place over a year
                  ago, while François's interview occurred only a few days ago.
                  So, it's possible that Ilya's views have changed in the past
                  year. However, it's clear that Ilya is more bullish on LLM's
                  than Chollet, so the majority (if not all) of his opinions
                  probably still stand.
                </p>

                <h2>Sutskever's Interview</h2>
                <p>
                  Early in the interview, Dwarkesh asks Ilya, "What would it
                  take to surpass human performance?" Right off the bat, Ilya
                  "challenges the claim that next-token prediction is not enough
                  to surpass human performance". He argues against the premise
                  that LLM's just learn to imitate, giving a straightforward
                  counter argument.
                </p>
                <blockquote class="blockquote">
                  "If your base neural net is smart enough. You just ask it —
                  What would a person with great insight, wisdom, and capability
                  do?
                  <b
                    >Maybe such a person doesn't exist, but there's a pretty
                    good chance the neural net would be able to extrapolate how
                    such a person would behave.</b
                  >"
                </blockquote>

                <p>
                  Ilya seems to believe that LLM's are so good at generalization
                  that they might even extrapolate the behavior of a "super"
                  person that doesn't actually exist. Before ChatGPT was
                  revealed to the world, almost nobody would have believed such
                  a claim. But now, there seems to be a large number of MLE's
                  and AI enthusiasts that believe in LLM's strong generalizing
                  ability.
                </p>

                <p>
                  Ilya goes on to discuss the implications of next-token
                  prediction's success. He says, "what does it mean to predict
                  the next token well enough? It's actually a deeper question
                  than it seems. Predicting the next token well means that you
                  understand the underlying reality that led to the creation of
                  that token." Ilya doesn't explicitly define "understanding" in
                  this context, but it's clear he suggests that LLM's are not
                  entirely limited by their input data.
                </p>

                <h2>Chollet's Interview</h2>
                <p>
                  While Ilya touched on LLM's and AGI in his interview, the
                  majority of François Chollet's interview seemed to revolve
                  around AGI and LLM's capabilities (or lack thereof). Within
                  the first couple of minutes, Chollet explains how he believes
                  LLM's work. "If you look at the way LLMs work is that they're
                  basically this big interpretive memory. The way you scale up
                  their capabilities is by trying to cram as much knowledge and
                  patterns as possible into them."
                </p>

                <p>
                  While this statement doesn't <em>directly</em> contradict
                  Ilya's claims, it's clear that Chollet is less impressed by
                  LLM's ability to generalize. Later in the interview, he goes
                  into more depth on this topic.
                </p>

                <blockquote class="blockquote">
                  "If you <b>scale up your database</b>, and you keep adding to
                  it more knowledge, more program templates
                  <b>then sure it becomes more and more skillful</b>. You can
                  apply it to more and more tasks.
                  <b
                    >But general intelligence is not task specific skills scaled
                    up to many skills, because there is an infinite space of
                    possible skills.</b
                  >
                </blockquote>

                <p>
                  Now <i>this</i> directly contradicts Ilya's claim that LLM's
                  can extrapolate the behavior of a "super" person. Chollet
                  continues by explaining what general intelligence
                  <i>actually</i> is.
                </p>

                <blockquote class="blockquote">
                  General intelligence is the ability to approach any problem,
                  any skill, and very quickly master it using very little data.
                  This is what makes you able to face anything you might ever
                  encounter. This is the definition of generality. Generality is
                  not specificity scaled up. It is the ability to apply your
                  mind to anything at all, to arbitrary things. This
                  fundamentally requires the ability to adapt, to learn on the
                  fly efficiently."
                </blockquote>

                <p>
                  Chollet argues that general intelligence cannot be achieved
                  merely by scaling up models or training data. He explains that
                  more data increases a model's"skill" but does not bestow
                  general intelligence. I love the way he succintly summarizes
                  this: "Generality is not specificity scale up."
                </p>

                <h2>Conclusion</h2>
                <p>
                  On one hand, we have Sutskever, who thinks that it might be
                  possible for general intelligence to be achieved through
                  scaling alone. On the other, we have Chollet, who outright
                  says that scaling is insufficient to achieve AGI. It's evident
                  that even top AI researchers cannot agree on the correct path
                  to AGI.
                </p>

                <p>
                  Estimates for achieving AGI vary widely, from a few years to
                  several decades. Chollet even suggests that LLMs have
                  <em>slowed down</em> progress towards AGI, stating, "OpenAI
                  basically set back progress towards AGI by quite a few years,
                  probably like 5-10 years." Only time will reveal whose
                  perspective is closer to the truth.
                </p>

                <div class="dots">...</div>
                <br />

                <p>
                  If you haven't already, I highly recommend listening to both
                  podcasts. Here's the
                  <a href="https://www.youtube.com/watch?v=Yf1o0TQzry8"
                    >link to Ilya's
                  </a>
                  and here's a
                  <a href="https://www.youtube.com/watch?v=UakqL6Pj9xo"
                    >link to François's.</a
                  >
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
